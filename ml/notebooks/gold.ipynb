{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6e0045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fichier trouvé à : /home/saida/Smart-LogiTrack/ml/notebooks/postgresql-42.6.0.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/14 15:28:38 WARN Utils: Your hostname, DESKTOP-A9FN519, resolves to a loopback address: 127.0.1.1; using 172.30.212.146 instead (on interface eth0)\n",
      "26/01/14 15:28:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "26/01/14 15:28:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+------------------+-----------+-----------+-----+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|     trip_duration|pickup_hour|day_of_week|month|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+------------------+-----------+-----------+-----+\n",
      "|       2| 2025-01-10 13:48:20|  2025-01-10 14:18:45|              1|         9.55|         1|                 N|         138|           4|           1|       41.5|  5.0|    0.5|     10.25|         0.0|                  1.0|       63.25|                 2.5|       1.75|              0.75|30.416666666666668|         13|          6|    1|\n",
      "|       2| 2025-01-10 13:28:44|  2025-01-10 13:46:19|              2|         3.03|         1|                 N|         211|         230|           1|       17.7|  0.0|    0.5|      4.49|         0.0|                  1.0|       26.94|                 2.5|        0.0|              0.75|17.583333333333332|         13|          6|    1|\n",
      "|       2| 2025-01-10 13:53:21|  2025-01-10 14:10:16|              1|         1.22|         1|                 N|          43|         170|           1|       14.9|  0.0|    0.5|      1.96|         0.0|                  1.0|       21.61|                 2.5|        0.0|              0.75|16.916666666666668|         13|          6|    1|\n",
      "|       2| 2025-01-10 13:30:58|  2025-01-10 13:32:03|              2|         0.54|         5|                 N|         219|          10|           1|       84.0|  0.0|    0.0|       0.0|         0.0|                  1.0|        85.0|                 0.0|        0.0|               0.0|1.0833333333333333|         13|          6|    1|\n",
      "|       2| 2025-01-10 13:09:20|  2025-01-10 13:30:06|              1|         4.35|         1|                 N|         233|         236|           1|       24.0|  0.0|    0.5|      5.75|         0.0|                  1.0|        34.5|                 2.5|        0.0|              0.75|20.766666666666666|         13|          6|    1|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+------------------+-----------+-----------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "jar_name = \"postgresql-42.6.0.jar\"\n",
    "jar_path = os.path.abspath(jar_name)\n",
    "\n",
    "if not os.path.exists(jar_path):\n",
    "    print(f\"❌ Le fichier est toujours absent du dossier {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"✅ Fichier trouvé à : {jar_path}\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgresFinalTest\") \\\n",
    "    .config(\"spark.jars\", jar_path) \\\n",
    "    .config(\"spark.driver.extraClassPath\", jar_path) \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "url = \"jdbc:postgresql://localhost:5432/silver_data\"\n",
    "properties = {\n",
    "        \"user\": \"silver_user\",\n",
    "        \"password\": \"silver_pass123\", # Vérifiez bien ce mot de passe dans votre docker-compose\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "    \n",
    "# Charger les données depuis Postgres vers un nouveau DataFrame\n",
    "df_silver = spark.read.jdbc(url=url, table=\"silver_data\", properties=properties)\n",
    "\n",
    "# Afficher les 5 premières lignes\n",
    "df_silver.show(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116be3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/13 20:11:06 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+-----------------+-------------------+---------------------+------------------+--------------------+------------------+-------------------+--------------------+------------------+------------------+------------------+\n",
      "|summary|           VendorID|   passenger_count|     trip_distance|        RatecodeID|store_and_fwd_flag|      PULocationID|      DOLocationID|       payment_type|       fare_amount|             extra|            mta_tax|       tip_amount|       tolls_amount|improvement_surcharge|      total_amount|congestion_surcharge|       Airport_fee| cbd_congestion_fee|       trip_duration|       pickup_hour|       day_of_week|             month|\n",
      "+-------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+-----------------+-------------------+---------------------+------------------+--------------------+------------------+-------------------+--------------------+------------------+------------------+------------------+\n",
      "|  count|            2616166|           2616166|           2616166|           2616166|           2616166|           2616166|           2616166|            2616166|           2616166|           2616166|            2616166|          2616166|            2616166|              2616166|           2616166|             2616166|           2616166|            2616166|             2616166|           2616166|           2616166|           2616166|\n",
      "|   mean| 1.7851218156646023|1.3033859472220035|2.3943842172094825|1.0264172839185282|              NULL|169.09783706385605|167.77912716547803| 1.1727596796227762| 14.89835961097442|1.5874861648687424| 0.4974467407649209|3.137796909676124|0.28341573508664486|    0.999998471045033|23.738469156799606|    2.38756638531347|0.0818900635510132| 0.4980621451391081|  12.305049762387128|14.420280670263278| 4.248650506122318|1.0000802701357636|\n",
      "| stddev|0.41073789053109416|0.7410251861324242|2.6858145100963977|0.2752339492141847|              NULL| 63.21477825916603| 68.14728631103853|0.47095759203598986|10.927851726555936|1.8332206799702482|0.03574671198658447|2.863951956681196| 1.5145323253865033| 0.001236508476981...|14.350297700027784|  0.5181146799832163| 0.369596323062116|0.35423263033970426|   7.107145049304279| 5.569371356815028|1.8981282662109502|0.0296503456763943|\n",
      "|    min|                  1|                 1|               0.1|                 1|                 N|                 1|                 1|                  1|               2.5|               0.0|                0.0|              0.0|                0.0|                  0.0|               4.0|                 0.0|               0.0|                0.0|0.016666666666666666|                 0|                 1|                 1|\n",
      "|    max|                  2|                 6|             47.05|                 6|                 Y|               265|               265|                  4|             198.0|              15.0|               4.75|            100.0|               45.0|                  1.0|            235.75|                 2.5|              1.75|               0.75|               34.25|                23|                 7|                12|\n",
      "+-------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+-----------------+-------------------+---------------------+------------------+--------------------+------------------+-------------------+--------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_silver.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc1f9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Features sélectionnées: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset prêt avec 2,616,166 lignes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "features_to_keep = [\n",
    "    'trip_distance',\n",
    "    'RatecodeID',\n",
    "    'tolls_amount',\n",
    "    'fare_amount',\n",
    "    'tip_amount',\n",
    "    'total_amount',\n",
    "    'Airport_fee',\n",
    "    'pickup_hour',\n",
    "    'day_of_week',\n",
    "    \"trip_duration\"\n",
    "]\n",
    "\n",
    "\n",
    "print(f\"\\n✅ Features sélectionnées: {len(features_to_keep) - 1}\")\n",
    "print(f\"✅ Dataset prêt avec {df_silver.count():,} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "620c4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, round as spark_round\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor, LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95740896",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'trip_distance',\n",
    "    'RatecodeID',\n",
    "    'tolls_amount',\n",
    "    'fare_amount',\n",
    "    'tip_amount',\n",
    "    'total_amount',\n",
    "    'Airport_fee',\n",
    "    'pickup_hour',\n",
    "    'day_of_week',\n",
    "    \n",
    "]\n",
    "target_column = 'trip_duration'\n",
    "df_Gold = df_silver.select(feature_columns + [target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c10e5281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples d'entraînement: 2092729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples de test: 523437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data, test_data = df_Gold.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Nombre d'exemples d'entraînement: {train_data.count()}\")\n",
    "print(f\"Nombre d'exemples de test: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6604ad04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[trip_distance: double, RatecodeID: int, tolls_amount: double, fare_amount: double, tip_amount: double, total_amount: double, Airport_fee: double, pickup_hour: int, day_of_week: int, trip_duration: double]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c9bfdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol='features_raw'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c93f59af",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(\n",
    "    inputCol='features_raw',\n",
    "    outputCol='features',\n",
    "    withStd=True,\n",
    "    withMean=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54f4e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LinearRegression(\n",
    "#     featuresCol='features',\n",
    "#     labelCol=target_column,\n",
    "#     maxIter=10,\n",
    "#     regParam=0.3,\n",
    "#     elasticNetParam=0.8\n",
    "# )\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol='features',\n",
    "    labelCol=target_column,\n",
    "    numTrees=50,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fbea3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline(stages=[assembler, scaler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e35d48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/12 14:39:51 WARN DAGScheduler: Broadcasting large task binary with size 1047.7 KiB\n",
      "26/01/12 14:41:54 WARN DAGScheduler: Broadcasting large task binary with size 2028.5 KiB\n",
      "26/01/12 14:44:44 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "26/01/12 14:48:02 WARN DAGScheduler: Broadcasting large task binary with size 1197.9 KiB\n",
      "26/01/12 14:48:16 WARN DAGScheduler: Broadcasting large task binary with size 7.3 MiB\n",
      "26/01/12 14:51:52 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_lr = pipeline_lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eddd46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr = model_lr.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d872c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_mae = RegressionEvaluator(labelCol=target_column, predictionCol='prediction', metricName='mae')\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=target_column, predictionCol='prediction', metricName='rmse')\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=target_column, predictionCol='prediction', metricName='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a811e8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mae_lr = evaluator_mae.evaluate(predictions_lr)\n",
    "rmse_lr = evaluator_rmse.evaluate(predictions_lr)\n",
    "r2_lr = evaluator_r2.evaluate(predictions_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1843028d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ RÉSULTATS - Régression Linéaire:\n",
      "   MAE:  1.09 minutes\n",
      "   RMSE: 1.76 minutes\n",
      "   R²:   0.9374\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n✅ RÉSULTATS - Régression Linéaire:\")\n",
    "print(f\"   MAE:  {mae_lr:.2f} minutes\")\n",
    "print(f\"   RMSE: {rmse_lr:.2f} minutes\")\n",
    "print(f\"   R²:   {r2_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe233a2",
   "metadata": {},
   "source": [
    "### GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84acae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=target_column,\n",
    "    maxIter=50,        # nombre d'arbres\n",
    "    maxDepth=6,        # profondeur (⚠️ augmente = plus lent)\n",
    "    stepSize=0.1,      # learning rate\n",
    "    subsamplingRate=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline_gbt = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "\n",
    "model_gbt = pipeline_gbt.fit(train_data)\n",
    "\n",
    "\n",
    "predictions_gbt = model_gbt.transform(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bc0f5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ RÉSULTATS - GBT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/12 15:11:36 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE  : 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 1.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 650:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²   : 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator_mae = RegressionEvaluator(\n",
    "    labelCol=target_column,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=target_column,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=target_column,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ RÉSULTATS - GBT\")\n",
    "print(f\"MAE  : {evaluator_mae.evaluate(predictions_gbt):.2f}\")\n",
    "print(f\"RMSE : {evaluator_rmse.evaluate(predictions_gbt):.2f}\")\n",
    "print(f\"R²   : {evaluator_r2.evaluate(predictions_gbt):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "682d8775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sauvegarder le modèle\n",
    "model_gbt.write().overwrite().save('../models/eta_spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83cd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------------+-----------+----------+------------+-----------+-----------+-----------+--------------------+-----------------+\n",
      "|trip_distance|RatecodeID|tolls_amount|fare_amount|tip_amount|total_amount|Airport_fee|pickup_hour|day_of_week|            features|       prediction|\n",
      "+-------------+----------+------------+-----------+----------+------------+-----------+-----------+-----------+--------------------+-----------------+\n",
      "|         9.55|         1|           0|       41.5|     10.25|       63.25|       1.75|         13|          6|[9.55,1.0,0.0,41....|26.23872154637569|\n",
      "+-------------+----------+------------+-----------+----------+------------+-----------+-----------+-----------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       prediction|\n",
      "+-----------------+\n",
      "|26.23872154637569|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.23872154637569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df = spark.createDataFrame([\n",
    "    (9.55, 1, 0, 41.5, 10.25, 63.25, 1.75, 13, 6)\n",
    "],\n",
    "[\n",
    "    'trip_distance',\n",
    "    'RatecodeID',\n",
    "    'tolls_amount',\n",
    "    'fare_amount',\n",
    "    'tip_amount',\n",
    "    'total_amount',\n",
    "    'Airport_fee',\n",
    "    'pickup_hour',\n",
    "    'day_of_week'\n",
    "])\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "loaded_model = PipelineModel.load(\"../models/eta_spark\")\n",
    "\n",
    "predictions = loaded_model.transform(test_df)\n",
    "predictions.show()\n",
    "predictions.select(\"prediction\").show()\n",
    "pred_value = predictions.select(\"prediction\").collect()[0][\"prediction\"]\n",
    "print(pred_value)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
